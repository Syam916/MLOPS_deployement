{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-05 19:20:50,117: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/repos/Syam916/mlops \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"Syam916/mlops\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"Syam916/mlops\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-05 19:20:50,125: INFO: helpers: Initialized MLflow to track repo \"Syam916/mlops\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository Syam916/mlops initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository Syam916/mlops initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-05 19:20:50,128: INFO: helpers: Repository Syam916/mlops initialized!]\n",
      "[2025-01-05 19:20:50,159: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-01-05 19:20:50,163: INFO: common: created directory at: artifacts]\n",
      "[2025-01-05 19:20:50,166: INFO: common: created directory at: artifacts/model_evaluation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/05 19:20:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-05 19:21:00,195: INFO: 4189970535: Model evaluation completed and metrics logged to MLflow]\n",
      "[2025-01-05 19:21:00,198: INFO: 4189970535: Evaluation results saved locally]\n",
      "🏃 View run aged-frog-414 at: https://dagshub.com/Syam916/mlops.mlflow/#/experiments/0/runs/48afb572de2d4b1a8852b23c33964ec5\n",
      "🧪 View experiment at: https://dagshub.com/Syam916/mlops.mlflow/#/experiments/0\n",
      "\n",
      "Model Performance Metrics:\n",
      "R² Score: 0.6466\n",
      "RMSE: $69,139.30\n",
      "MAE: $49,924.92\n",
      "\n",
      "Model Type: Ridge\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os,json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "from src.constants import *\n",
    "from src.utils.common import *\n",
    "import dagshub\n",
    "dagshub.init(repo_owner='Syam916', repo_name='mlops', mlflow=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_transformed_path: Path\n",
    "    test_target_path: Path\n",
    "    best_model_path: Path\n",
    "    evaluation_results_path: Path\n",
    "    mlflow_uri: str\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config=CONFIG_FILE_PATH):\n",
    "        self.config = read_yml_file(CONFIG_FILE_PATH)\n",
    "        create_directories([self.config.artifacts_directory])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_transformed_path=config.test_data_transformed_path,\n",
    "            test_target_path=config.test_target_path,\n",
    "            best_model_path=config.best_model_path,\n",
    "            evaluation_results_path=config.evaluation_results_path,\n",
    "            mlflow_uri=config.mlflow_uri\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate the best model and log metrics to MLflow\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load test data and best model\n",
    "            X_test = np.load(self.config.test_data_transformed_path)\n",
    "            y_test = np.load(self.config.test_target_path)\n",
    "\n",
    "            with open(self.config.best_model_path, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "\n",
    "            # Set MLflow tracking URI\n",
    "            mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "            mlflow.set_experiment(\"house_price_prediction\")\n",
    "\n",
    "            with mlflow.start_run():\n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "                # Log metrics to MLflow\n",
    "                mlflow.log_metric(\"r2_score\", r2)\n",
    "                mlflow.log_metric(\"rmse\", rmse)\n",
    "                mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "                # Log model parameters\n",
    "                model_params = model.get_params()\n",
    "                mlflow.log_params(model_params)\n",
    "\n",
    "                # Log model name and type\n",
    "                mlflow.log_param(\"model_type\", type(model).__name__)\n",
    "\n",
    "                # Log the model itself\n",
    "                mlflow.sklearn.log_model(model, \"best_model\")\n",
    "\n",
    "                logger.info(\"Model evaluation completed and metrics logged to MLflow\")\n",
    "\n",
    "                # Save evaluation results locally\n",
    "                results = {\n",
    "                    'R2_Score': r2,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAE': mae,\n",
    "                    'Model_Type': type(model).__name__,\n",
    "                    'Model_Parameters': model_params\n",
    "                }\n",
    "\n",
    "                with open(self.config.evaluation_results_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "\n",
    "                logger.info(\"Evaluation results saved locally\")\n",
    "                \n",
    "                return results\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in model evaluation: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        config = ConfigurationManager()\n",
    "        model_evaluation_config = config.get_model_evaluation_config()\n",
    "        model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "        \n",
    "        evaluation_results = model_evaluation.evaluate_model()\n",
    "        \n",
    "        print(\"\\nModel Performance Metrics:\")\n",
    "        print(f\"R² Score: {evaluation_results['R2_Score']:.4f}\")\n",
    "        print(f\"RMSE: ${evaluation_results['RMSE']:,.2f}\")\n",
    "        print(f\"MAE: ${evaluation_results['MAE']:,.2f}\")\n",
    "        print(f\"\\nModel Type: {evaluation_results['Model_Type']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
