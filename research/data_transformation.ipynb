{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-05 17:33:42,287: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-01-05 17:33:42,299: INFO: common: created directory at: artifacts]\n",
      "[2025-01-05 17:33:42,302: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-01-05 17:33:42,344: INFO: 979195368: Loaded train and test data]\n",
      "[2025-01-05 17:33:42,386: INFO: 979195368: Split data into features and target]\n",
      "[2025-01-05 17:33:42,448: INFO: 979195368: Transformed training and test data]\n",
      "[2025-01-05 17:33:42,457: INFO: 979195368: Saved preprocessor and transformed data]\n",
      "Transformed training data shape: (15480, 12)\n",
      "Transformed test data shape: (5160, 12)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "from src.constants import *\n",
    "from src.utils.common import *\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    preprocessor_path: Path\n",
    "    transformed_train_path: Path\n",
    "    transformed_test_path: Path\n",
    "    train_target_path: Path\n",
    "    test_target_path: Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config=CONFIG_FILE_PATH):\n",
    "        self.config = read_yml_file(CONFIG_FILE_PATH)\n",
    "        create_directories([self.config.artifacts_directory])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path,\n",
    "            preprocessor_path=config.preprocessor_path,\n",
    "            transformed_train_path=config.transformed_train_path,\n",
    "            transformed_test_path=config.transformed_test_path,\n",
    "            train_target_path=config.train_target_path,\n",
    "            test_target_path=config.test_target_path\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def get_data_transformer(self, train_data):\n",
    "        \"\"\"\n",
    "        Create preprocessing pipeline for numerical and categorical data\n",
    "        \"\"\"\n",
    "        # Separate numerical and categorical columns\n",
    "        numerical_columns = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_columns = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        # Remove target variable from numerical columns\n",
    "        if 'median_house_value' in numerical_columns:\n",
    "            numerical_columns.remove('median_house_value')\n",
    "        \n",
    "        # Create preprocessing pipelines\n",
    "        num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        cat_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(drop='first'))\n",
    "        ])\n",
    "        \n",
    "        # Combine pipelines\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num_transform', num_pipeline, numerical_columns),\n",
    "            ('cat_transform', cat_pipeline, categorical_columns)\n",
    "        ])\n",
    "        \n",
    "        return preprocessor\n",
    "    \n",
    "    def transform_data(self):\n",
    "        \"\"\"\n",
    "        Transform the training and test data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the training and test data\n",
    "            train_data = pd.read_csv(self.config.train_data_path)\n",
    "            test_data = pd.read_csv(self.config.test_data_path)\n",
    "            \n",
    "            logger.info(\"Loaded train and test data\")\n",
    "            \n",
    "            # Create preprocessor\n",
    "            preprocessor = self.get_data_transformer(train_data)\n",
    "            \n",
    "            # Separate features and target\n",
    "            X_train = train_data.drop('median_house_value', axis=1)\n",
    "            y_train = train_data['median_house_value']\n",
    "            X_test = test_data.drop('median_house_value', axis=1)\n",
    "            y_test = test_data['median_house_value']\n",
    "            \n",
    "            logger.info(\"Split data into features and target\")\n",
    "            \n",
    "            # Fit and transform training data\n",
    "            X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "            X_test_transformed = preprocessor.transform(X_test)\n",
    "            \n",
    "            logger.info(\"Transformed training and test data\")\n",
    "            \n",
    "            # Save preprocessor\n",
    "            with open(self.config.preprocessor_path, 'wb') as f:\n",
    "                pickle.dump(preprocessor, f)\n",
    "            \n",
    "            # Save transformed data\n",
    "            np.save(self.config.transformed_train_path, X_train_transformed)\n",
    "            np.save(self.config.transformed_test_path, X_test_transformed)\n",
    "            np.save(self.config.train_target_path, y_train)\n",
    "            np.save(self.config.test_target_path, y_test)\n",
    "            \n",
    "            logger.info(\"Saved preprocessor and transformed data\")\n",
    "            \n",
    "            return (\n",
    "                X_train_transformed,\n",
    "                X_test_transformed,\n",
    "                y_train,\n",
    "                y_test\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in data transformation: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    \n",
    "    X_train_transformed, X_test_transformed, y_train, y_test = data_transformation.transform_data()\n",
    "    \n",
    "    print(\"Transformed training data shape:\", X_train_transformed.shape)\n",
    "    print(\"Transformed test data shape:\", X_test_transformed.shape)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
